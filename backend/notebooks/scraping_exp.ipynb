{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# change the working directory to root of this project\n",
    "os.chdir(\"..\")\n",
    "# os.chdir(\"..\")\n",
    "\n",
    "# verify the working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.etl.extract.site_scraper import LinkedinScraper, IndeedScraper, JobbermanScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 18:41:56,004:site_scraper.py:__init__:INFO:Initializing LinkedinScraper:\n",
      "2023-11-27 18:41:56,185:site_scraper.py:__init__:INFO:Setting up webdriver:\n",
      "2023-11-27 18:41:58,740:site_scraper.py:__init__:INFO:webdriver setup successful:\n",
      "2023-11-27 18:42:07,224:site_scraper.py:get_jobs:INFO:Scraping 25 jobs from LinkedIn:\n",
      "/home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend/etl/extract/site_scraper.py:366: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 366 of the file /home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend/etl/extract/site_scraper.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  job_description = bs(job_description[0].text).text\n",
      "/home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend/etl/extract/site_scraper.py:374: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 374 of the file /home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend/etl/extract/site_scraper.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  seniority_level = bs(job_details[0].text).text\n",
      "/home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend/etl/extract/site_scraper.py:378: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 378 of the file /home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend/etl/extract/site_scraper.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  employment_type = bs(job_details[1].text).text\n",
      "/home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend/etl/extract/site_scraper.py:382: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 382 of the file /home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend/etl/extract/site_scraper.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  job_function = bs(job_details[2].text).text\n",
      "/home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend/etl/extract/site_scraper.py:386: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 386 of the file /home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend/etl/extract/site_scraper.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  industries = bs(job_details[3].text).text\n",
      "2023-11-27 18:42:27,334:site_scraper.py:get_job_details:INFO:Job 1 page scraped successfully:\n",
      "2023-11-27 18:42:29,876:site_scraper.py:get_job_details:INFO:Job 2 page scraped successfully:\n",
      "2023-11-27 18:42:36,360:site_scraper.py:get_job_details:INFO:Job 3 page scraped successfully:\n",
      "2023-11-27 18:42:39,618:site_scraper.py:get_job_details:INFO:Job 4 page scraped successfully:\n",
      "2023-11-27 18:42:42,599:site_scraper.py:get_job_details:INFO:Job 5 page scraped successfully:\n",
      "2023-11-27 18:42:45,378:site_scraper.py:get_job_details:INFO:Job 6 page scraped successfully:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an error scraping Page  7 :  HTTP Error 429: Request denied\n",
      "There was an error scraping Page  8 :  HTTP Error 429: Request denied\n",
      "There was an error scraping Page  9 :  HTTP Error 429: Request denied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 18:42:50,519:site_scraper.py:get_job_details:INFO:Job 10 page scraped successfully:\n",
      "2023-11-27 18:42:52,806:site_scraper.py:get_job_details:INFO:Job 11 page scraped successfully:\n",
      "2023-11-27 18:42:55,533:site_scraper.py:get_job_details:INFO:Job 12 page scraped successfully:\n",
      "2023-11-27 18:42:58,203:site_scraper.py:get_job_details:INFO:Job 13 page scraped successfully:\n",
      "2023-11-27 18:43:01,580:site_scraper.py:get_job_details:INFO:Job 14 page scraped successfully:\n",
      "2023-11-27 18:43:04,522:site_scraper.py:get_job_details:INFO:Job 15 page scraped successfully:\n",
      "2023-11-27 18:43:07,162:site_scraper.py:get_job_details:INFO:Job 16 page scraped successfully:\n",
      "2023-11-27 18:43:09,856:site_scraper.py:get_job_details:INFO:Job 17 page scraped successfully:\n",
      "2023-11-27 18:43:12,403:site_scraper.py:get_job_details:INFO:Job 18 page scraped successfully:\n",
      "2023-11-27 18:43:14,565:site_scraper.py:get_job_details:INFO:Job 19 page scraped successfully:\n",
      "2023-11-27 18:43:17,334:site_scraper.py:get_job_details:INFO:Job 20 page scraped successfully:\n",
      "/home/abraham-pc/Documents/personal_projects/Multi-Platform_Job_Recommender/backend/etl/extract/site_scraper.py:382: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  job_function = bs(job_details[2].text).text\n",
      "2023-11-27 18:43:19,979:site_scraper.py:get_job_details:INFO:Job 21 page scraped successfully:\n",
      "2023-11-27 18:43:22,925:site_scraper.py:get_job_details:INFO:Job 22 page scraped successfully:\n",
      "2023-11-27 18:43:25,472:site_scraper.py:get_job_details:INFO:Job 23 page scraped successfully:\n",
      "2023-11-27 18:43:28,450:site_scraper.py:get_job_details:INFO:Job 24 page scraped successfully:\n",
      "2023-11-27 18:43:31,202:site_scraper.py:get_job_details:INFO:Job 25 page scraped successfully:\n",
      "2023-11-27 18:43:33,949:site_scraper.py:get_job_details:INFO:Job 26 page scraped successfully:\n",
      "2023-11-27 18:43:36,740:site_scraper.py:get_job_details:INFO:Job 27 page scraped successfully:\n",
      "2023-11-27 18:43:39,582:site_scraper.py:get_job_details:INFO:Job 28 page scraped successfully:\n",
      "2023-11-27 18:43:43,012:site_scraper.py:get_job_details:INFO:Job 29 page scraped successfully:\n",
      "2023-11-27 18:43:45,378:site_scraper.py:get_job_details:INFO:Job 30 page scraped successfully:\n",
      "2023-11-27 18:43:47,792:site_scraper.py:get_job_details:INFO:Job 31 page scraped successfully:\n",
      "2023-11-27 18:43:50,359:site_scraper.py:get_job_details:INFO:Job 32 page scraped successfully:\n",
      "2023-11-27 18:43:52,976:site_scraper.py:get_job_details:INFO:Job 33 page scraped successfully:\n",
      "2023-11-27 18:43:55,927:site_scraper.py:get_job_details:INFO:Job 34 page scraped successfully:\n",
      "2023-11-27 18:43:58,665:site_scraper.py:get_job_details:INFO:Job 35 page scraped successfully:\n",
      "2023-11-27 18:44:01,012:site_scraper.py:get_job_details:INFO:Job 36 page scraped successfully:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an error scraping Page  37 :  HTTP Error 429: Request denied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 18:44:04,724:site_scraper.py:get_job_details:INFO:Job 38 page scraped successfully:\n",
      "2023-11-27 18:44:07,333:site_scraper.py:get_job_details:INFO:Job 39 page scraped successfully:\n",
      "2023-11-27 18:44:10,040:site_scraper.py:get_job_details:INFO:Job 40 page scraped successfully:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an error scraping Page  41 :  HTTP Error 429: Request denied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 18:44:13,573:site_scraper.py:get_job_details:INFO:Job 42 page scraped successfully:\n",
      "2023-11-27 18:44:16,248:site_scraper.py:get_job_details:INFO:Job 43 page scraped successfully:\n",
      "2023-11-27 18:44:18,910:site_scraper.py:get_job_details:INFO:Job 44 page scraped successfully:\n",
      "2023-11-27 18:44:24,823:site_scraper.py:get_job_details:INFO:Job 45 page scraped successfully:\n",
      "2023-11-27 18:44:30,117:site_scraper.py:get_job_details:INFO:Job 46 page scraped successfully:\n",
      "2023-11-27 18:44:36,390:site_scraper.py:get_job_details:INFO:Job 47 page scraped successfully:\n",
      "2023-11-27 18:44:39,218:site_scraper.py:get_job_details:INFO:Job 48 page scraped successfully:\n",
      "2023-11-27 18:44:41,684:site_scraper.py:get_job_details:INFO:Job 49 page scraped successfully:\n",
      "2023-11-27 18:44:44,486:site_scraper.py:get_job_details:INFO:Job 50 page scraped successfully:\n",
      "2023-11-27 18:44:44,647:site_scraper.py:create_dataframe:INFO:Creating jobs dataframe:\n",
      "2023-11-27 18:44:44,650:site_scraper.py:update_database:INFO:Updating database:\n",
      "2023-11-27 18:44:44,650:site_scraper.py:create_dataframe:INFO:Creating jobs dataframe:\n",
      "2023-11-27 18:44:44,652:site_scraper.py:update_database:INFO:Found 50 new jobs:\n",
      "2023-11-27 18:44:44,655:site_scraper.py:update_database:INFO:Adding 50 new jobs to job_listings table:\n",
      "2023-11-27 18:44:45,170:site_scraper.py:update_database:INFO:50 new jobs added successfully:\n"
     ]
    }
   ],
   "source": [
    "# indeed_scraper = IndeedScraper()\n",
    "# scraping = indeed_scraper.scrape()\n",
    "# jobs_df = indeed_scraper.create_dataframe()\n",
    "# indeed_scraper.update_database() # working\n",
    "\n",
    "linkedin_scraper = LinkedinScraper()\n",
    "scraping = linkedin_scraper.scrape()\n",
    "jobs_df = linkedin_scraper.create_dataframe()\n",
    "linkedin_scraper.update_database()\n",
    "\n",
    "# jobberman_scraper = JobbermanScraper()\n",
    "# scraping = jobberman_scraper.scrape()\n",
    "# jobs_df = jobberman_scraper.create_dataframe()\n",
    "# jobberman_scraper.update_database()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = linkedin_scraper.create_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['job_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(jobs_df['job_desc'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Keeper\n",
    ".css-1p3gyjy > div:nth-child(1) > div:nth-child(1)\n",
    "\n",
    "# Laundry Supervisor\n",
    "div.css-1p3gyjy:nth-child(1) > div:nth-child(1) > div:nth-child(1)\n",
    "\n",
    "# Data Entry (correct)\n",
    "/html/body/div/div/div[2]/div/div/div[1]/div[3]/div/div[2]/div[4]/div[2]/div[2]/div/div/div/div\n",
    "\n",
    ".css-1p3gyjy > div:nth-child(1) > div:nth-child(1)\n",
    "div.css-1p3gyjy:nth-child(1) > div:nth-child(1) > div:nth-child(1)\n",
    ".css-tvvxwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".JobsList_jobsList__Ey2Vo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/html/body/div[2]/div[1]/div[3]/div[2]/div[1]/div[2]/ul/li[{n}]/div/div/div[1]/div[1]/a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.ceil(37 / 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.to_csv('sample_jobs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
